const ArgumentType = require('../../extension-support/argument-type');
const Cast = require('../../util/cast');
const BlockType = require('../../extension-support/block-type');
const color = require('../../util/color');
const log = require('../../util/log');

/**
 * Icon svg to be displayed at the left edge of each extension block, encoded as a data URI.
 * @type {string}
 */
// eslint-disable-next-line max-len
const iconURI = 'https://www.gstatic.com/images/icons/material/system/1x/mic_white_24dp.png'
const menuIconURI = 'https://www.gstatic.com/images/icons/material/system/1x/mic_grey600_24dp.png'

class Scratch3SpeechBlocks {
	constructor (runtime) {
    /**
     * The runtime instantiating this block package.
     * @type {Runtime}
     */
    this.runtime = runtime;

    // For seeding the recognition engine and for deciding whether to accept results.
    // TODO: pull these values out of the hat blocks inste of hard coding them.
    this._phrase_list = [];

    /**
     * The most recent result received from the speech API transcription.
     * @type {String}
     */
    this.current_utterance = null;

    // using this to test out hat blocks that edge trigger.  The reporter block
    // uses current_utterance and we probably? don't want to reset the value unless
    // we have new transcription results.  But, in order to detect someone saying
    // the same thing twice in two subsequent liten and wait blocks 
    // and still trigger the hat, we need this to go from
    // '' at the beginning of the listen to '<transcription value' at the end.
    this.temp_speech = null;

    this.match_result = null;

    /**
     * The list of queued `resolve` callbacks for speech'.
     * @type {!Array}
     */
    // Note. This a list because each block used to have its own chance to listen
    // e.g. each block listenend for an utterance.  I've switched it back so that
    // if two listen blocks try to start running at the same time,
    // there is only 1 utterance detected
    this._speechList = [];

    this._alreadyListening = false;

    this._speechTimeout = null;
    this._speechTimeoutResponseTimeout = null;

    // The ScriptProcessorNode hooked up to the audio context.
    this._scriptNode = null;

    // The socket to send microphone data over.
    this._socket = null;
    // The AudioContext used to manage the microphone
    this._context = null;
    // MediaStreamAudioSourceNode to handle microphone data.
    this._sourceNode = null;

    // A Promise whose fulfillment handler receives a MediaStream object when the microphone has been obtained.
    this._audioPromise = null;

    // Audio buffers for sounds to indicate that listending has started and ended.
    this._startSoundBuffer = null;
    this._endSoundBuffer = null;

    // Defaults.
    // Redefine these in your program to override the defaults.

    // At what point is no match declared (0.0 = perfection, 1.0 = very loose).
    this.Match_Threshold = 0.5;
    // How far to search for a match (0 = exact location, 1000+ = broad match).
    // A match this many characters away from the expected location will add
    // 1.0 to the score (0.0 is a perfect match).
    this.Match_Distance = 1000;
    
    // The number of bits in an int.
    this.Match_MaxBits = 32;
    this._loadUISounds();

    // Come back and figure out which of these I really need.
    this.startRecording = this.startRecording.bind(this);
    this._newWebsocket = this._newWebsocket.bind(this);
    this._newSocketCallback = this._newSocketCallback.bind(this);
    this._setupSocketCallback = this._setupSocketCallback.bind(this);
    this._socketMessageCallback = this._socketMessageCallback.bind(this);
    this._startByteStream = this._startByteStream.bind(this);
    this._processAudioCallback = this._processAudioCallback.bind(this);
    this._onTranscriptionFromServer = this._onTranscriptionFromServer.bind(this);
    this._timeOutListening = this._timeOutListening.bind(this);
    this._timeOutWaitingforFinal = this._timeOutWaitingforFinal.bind(this);
    this._startNextListening = this._startNextListening.bind(this);
    this._resetActiveListening = this._resetActiveListening.bind(this);


    this.runtime.on('TRANSCRIPTION', this._onTranscription.bind(this));
    this.runtime.on('PROJECT_STOP_ALL', this._resetListening.bind(this));
  }

  //  MATCH FUNCTIONS

  /**
   * Locate the best instance of 'pattern' in 'text' near 'loc'.
   * @param {string} text The text to search.
   * @param {string} pattern The pattern to search for.
   * @param {number} loc The location to search around.
   * @return {number} Best match index or -1.
   */
  match_main (text, pattern, loc) {
    // Check for null inputs.
    if (text == null || pattern == null || loc == null) {
      throw new Error('Null input. (match_main)');
    }

    loc = Math.max(0, Math.min(loc, text.length));
    if (text == pattern) {
      // Shortcut (potentially not guaranteed by the algorithm)
      return 0;
    } else if (!text.length) {
      // Nothing to match.
      return -1;
    } else if (text.substring(loc, loc + pattern.length) == pattern) {
      // Perfect match at the perfect spot!  (Includes case of null pattern)
      return loc;
    } else {
      // Do a fuzzy compare.
      return this.match_bitap_(text, pattern, loc);
    }
  };


  /**
   * Locate the best instance of 'pattern' in 'text' near 'loc' using the
   * Bitap algorithm.
   * @param {string} text The text to search.
   * @param {string} pattern The pattern to search for.
   * @param {number} loc The location to search around.
   * @return {number} Best match index or -1.
   * @private
   */
  match_bitap_ (text, pattern, loc) {
    if (pattern.length > this.Match_MaxBits) {
      throw new Error('Pattern too long for this browser.');
    }

    // Initialise the alphabet.
    var s = this.match_alphabet_(pattern);

    var dmp = this;  // 'this' becomes 'window' in a closure.

    /**
     * Compute and return the score for a match with e errors and x location.
     * Accesses loc and pattern through being a closure.
     * @param {number} e Number of errors in match.
     * @param {number} x Location of match.
     * @return {number} Overall score for match (0.0 = good, 1.0 = bad).
     * @private
     */
    function match_bitapScore_(e, x) {
      var accuracy = e / pattern.length;
      var proximity = Math.abs(loc - x);
      if (!dmp.Match_Distance) {
        // Dodge divide by zero error.
        return proximity ? 1.0 : accuracy;
      }
      return accuracy + (proximity / dmp.Match_Distance);
    }

    // Highest score beyond which we give up.
    var score_threshold = this.Match_Threshold;
    // Is there a nearby exact match? (speedup)
    var best_loc = text.indexOf(pattern, loc);
    if (best_loc != -1) {
      score_threshold = Math.min(match_bitapScore_(0, best_loc), score_threshold);
      // What about in the other direction? (speedup)
      best_loc = text.lastIndexOf(pattern, loc + pattern.length);
      if (best_loc != -1) {
        score_threshold =
            Math.min(match_bitapScore_(0, best_loc), score_threshold);
      }
    }

    // Initialise the bit arrays.
    var matchmask = 1 << (pattern.length - 1);
    best_loc = -1;

    var bin_min, bin_mid;
    var bin_max = pattern.length + text.length;
    var last_rd;
    for (var d = 0; d < pattern.length; d++) {
      // Scan for the best match; each iteration allows for one more error.
      // Run a binary search to determine how far from 'loc' we can stray at this
      // error level.
      bin_min = 0;
      bin_mid = bin_max;
      while (bin_min < bin_mid) {
        if (match_bitapScore_(d, loc + bin_mid) <= score_threshold) {
          bin_min = bin_mid;
        } else {
          bin_max = bin_mid;
        }
        bin_mid = Math.floor((bin_max - bin_min) / 2 + bin_min);
      }
      // Use the result from this iteration as the maximum for the next.
      bin_max = bin_mid;
      var start = Math.max(1, loc - bin_mid + 1);
      var finish = Math.min(loc + bin_mid, text.length) + pattern.length;

      var rd = Array(finish + 2);
      rd[finish + 1] = (1 << d) - 1;
      for (var j = finish; j >= start; j--) {
        // The alphabet (s) is a sparse hash, so the following line generates
        // warnings.
        var charMatch = s[text.charAt(j - 1)];
        if (d === 0) {  // First pass: exact match.
          rd[j] = ((rd[j + 1] << 1) | 1) & charMatch;
        } else {  // Subsequent passes: fuzzy match.
          rd[j] = (((rd[j + 1] << 1) | 1) & charMatch) |
                  (((last_rd[j + 1] | last_rd[j]) << 1) | 1) |
                  last_rd[j + 1];
        }
        if (rd[j] & matchmask) {
          var score = match_bitapScore_(d, j - 1);
          // This match will almost certainly be better than any existing match.
          // But check anyway.
          if (score <= score_threshold) {
            // Told you so.
            score_threshold = score;
            best_loc = j - 1;
            if (best_loc > loc) {
              // When passing loc, don't exceed our current distance from loc.
              start = Math.max(1, 2 * loc - best_loc);
            } else {
              // Already passed loc, downhill from here on in.
              break;
            }
          }
        }
      }
      // No hope for a (better) match at greater error levels.
      if (match_bitapScore_(d + 1, loc) > score_threshold) {
        break;
      }
      last_rd = rd;
    }
    return best_loc;
  }


  /**
   * Initialise the alphabet for the Bitap algorithm.
   * @param {string} pattern The text to encode.
   * @return {!Object} Hash of character locations.
   * @private
   */
  match_alphabet_ (pattern) {
    var s = {};
    for (var i = 0; i < pattern.length; i++) {
      s[pattern.charAt(i)] = 0;
    }
    for (var i = 0; i < pattern.length; i++) {
      s[pattern.charAt(i)] |= 1 << (pattern.length - i - 1);
    }
    return s;
  }

    /**
     * Download and decode the UI sounds.
     */
    _loadUISounds () {
        this._loadSound('speech-rec-start').then(buffer => {
            this._startSoundBuffer = buffer;
        });
        this._loadSound('speech-rec-end').then(buffer => {
            this._endSoundBuffer = buffer;
        });
    }

    /**
     * Download and decode a sound.
     * @param {string} fileName - the audio file name.
     * @return {Promise} - a promise which will resolve once the sound has loaded.
     */
    _loadSound (fileName) {
        if (!this.runtime.storage) return;
        if (!this.runtime.audioEngine) return;
        if (!this.runtime.audioEngine.audioContext) return;
        return this.runtime.storage.load(this.runtime.storage.AssetType.Sound, fileName, 'mp3')
            .then(soundAsset => {
                const context = this.runtime.audioEngine.audioContext;
                // Check for newer promise-based API
                if (context.decodeAudioData.length === 1) {
                    return context.decodeAudioData(soundAsset.data.buffer);
                } else { // eslint-disable-line no-else-return
                    // Fall back to callback API
                    return new Promise((resolve, reject) =>
                        context.decodeAudioData(soundAsset.data.buffer,
                            buffer => resolve(buffer),
                            error => reject(error)
                        )
                    );
                }
            });
    }
	/**
   * The key to load & store a target's speech-related state.
   * @type {string}
   */
  static get STATE_KEY () {
    return 'Scratch.speech';
  }

  // Resets all things related to listening. Called on Red Stop sign button.
  //   - suspends audio context
  //   - closes socket with speech socket server
  //   - clears out any remaining speech blocks that think they need to run.
  _resetListening() {
    console.log("_resetListening.");
    // Check whether context has been set up yet. This can get called before
 	  // We ever tried to listen for anything. e.g. on Green Flag click.
 	  if (this._context) {
 		  this._context.suspend.bind(this._context);
 	  }
 	  this._closeWebsocket();
 	  if (this._speechList.length > 0) {
 		  console.log('resetting so resolving everything');
   	    for (var i = 0; i < this._speechList.length; i++) {
	  	    var resFn = this._speechList[i];
	  	    resFn();
 	  	  }
 		  this._speechList = [];  // TODO does this actually resolve anything?
 	  }
  }

	// Called to reset a single instance of listening.  If there are utterances
	// expected in the queue, kick off the next one.
  _resetActiveListening() {
    console.log('resetting active listening');
    if (this._speechList.length > 0) {
      var resolve = this._speechList.shift();
   		// Pause the mic and close the web socket.
      this._context.suspend.bind(this._context);
      this._closeWebsocket();
      resolve();
      for (var i = 0; i < this._speechList.length; i++) {
        var resFn = this._speechList[i];
        resFn();
      }

   	  this._speechList = []; // reset list. CHECK WITH PROMISE IS RESOLVED
   	  this._alreadyListening = false;
    }
  }

  // Callback when ready to setup a new socket connection with speech server.
  _newSocketCallback(resolve, reject) {
    console.log('creating a new web socket');
  	// TODO: Stop hardcoding localhost and port
    //var server = 'ws://scratch-speech-prod.us-east-1.elasticbeanstalk.com';
    var server = 'ws://localhost:8080';
    this._socket = new WebSocket(server);
    this._socket.addEventListener('open', resolve);
    this._socket.addEventListener('error', reject);
  }

  _stopTranscription() {
    // what should actually get stopped here???
    if (this._socket) {
      this._context.suspend.bind(this._context);
      if (this._scriptNode) {
        this._scriptNode.disconnect();        
      }
      this._socket.send('stopPlease');
      // Give it a couple seconds to response before giving up and assuming nothing.
      this._speechTimeoutResponseTimeout = setTimeout(this._timeOutWaitingforFinal, 3000);

    }
  }

  _timeOutWaitingforFinal() {
    console.log('timeing out waiting for last response');
    this._resetActiveListening();
  }
	// Callback to handle initial setting up of a socket.
	// Currently we send a setup message (only contains sample rate) but might
	// be useful to send more data so we can do quota stuff.
	_setupSocketCallback(values) {
    this._micStream = values[0];
    this._socket = values[1].target;

      // TODO: go look at the serve and see if it implements this.
      this._socket.addEventListener('close', function(e) {
        console.log('socket close listener..');
      });
      this._socket.addEventListener('error', function(e) {
        console.log('Error from websocket', e);
      });

      // Send the initial configuration message. When the server acknowledges
      // it, start streaming the audio bytes to the server and listening for
      // transcriptions.
      this._socket.addEventListener('message', this._socketMessageCallback, {once: true});
      console.log('sending phrase list: ' + this._phrase_list);
      this._socket.send(JSON.stringify(
        {sampleRate: this._context.sampleRate,
         phrases: this._phrase_list,
        }
      ));
  }
	
  _scanBlocksForPhraseList() {
    var words = [];
    // For each each target, walk through the top level blocks and check whether
    // they are speech hat/when I hear blocks.
    this.runtime.targets.forEach(function(target) {
      target.blocks._scripts.forEach(function(id) {
        var b = target.blocks.getBlock(id);
        if (b.opcode === 'speech.whenIHearHat') {
          // Grab the text from the hat block's shadow.
          var inputId = b.inputs.PHRASE.block;
          var inputBlock = target.blocks.getBlock(inputId);
          var word = target.blocks.getBlock(inputId).fields.TEXT.value;
          words.push(word);          
        }
      })
    });
    this._phrase_list = words;
  }
	// Setup listening for socket.
	_socketMessageCallback(e) {
    console.log('socket message callback');
    this._socket.addEventListener('message', this._onTranscriptionFromServer);
    this._startByteStream(e);
  }

	// Setup so we can start streaming mic data
	_startByteStream(e) {
		// Hook up the scriptNode to the mic
    this._sourceNode = this._context.createMediaStreamSource(this._micStream);
    this._sourceNode.connect(this._scriptNode);
    this._scriptNode.connect(this._context.destination);
  }

	// Called when we're ready to start listening and want to open a socket.
	_newWebsocket() {
		console.log('setting up new socket and setting up block timeout.');
		var websocketPromise = new Promise(this._newSocketCallback); 
      Promise.all([this._audioPromise, websocketPromise]).then(
        this._setupSocketCallback).catch(console.log.bind(console));
	}
	
	// Called when we're done listening and want to close the web socket server.
	// Stops listening to the mic and whatnot as well.
	_closeWebsocket() {
	  console.log('closing socket');
	  // This is called on green flag to reset things that may never have existed
	  // in the first place. Do a bunch of checks.
    if (this._scriptNode) {
  	  this._scriptNode.disconnect();      	
    }
    if (this._sourceNode) this._sourceNode.disconnect();
    if (this._socket && this._socket.readyState === this._socket.OPEN) {
    	console.log('sending close socket message');
    	this._socket.close();
    } 
	}

	// Called when a listen block times out without detecting an end of
	// utterance message during transcription.
 _timeOutListening() {
 	  console.log('timeout fired. Resetting listening');
    //this.current_utterance = '';  // should this be NULL OR empty?
  //  this.temp_speech = ''; // should this be null or empty?
//    this._resetActiveListening();
  this._stopTranscription();
  this._playSound(this._endSoundBuffer);
 }

 // When we get a transcription result, save the result to current_utterance,
 // resolve the current promise.  
 _onTranscription (result) {
 	  var text = result.alternatives[0].transcript
    // Confidence seems to be 0 when a result has isFinal: true
 	  text = Cast.toString(text).toLowerCase();
    // facilitate matches by removing some punctuation: . ? !
    text = text.replace(/[.?!]/g, '');
    // trim off any white space
    text = text.trim();
    
    //this._computeMatch(text);

    var phrases = this._phrase_list.join(' ');
    var match = this._computeMatch(text, phrases)
    if (match != -1) {
      console.log('partial match.');
      this.match_result = text.substring(match, match + phrases.length)
      console.log('match result: ' + this.match_result);
    }
    var shouldKeepMatch =  match != -1 && result.stability > .85; // don't keep matches if the stability is low.

    //if (!result.isFinal && result.stability < .85 && !this._phrase_list.includes(text) && match == -1) {
    if (!result.isFinal  && !this._phrase_list.includes(text) && !shouldKeepMatch) {
      this._possible_result = text;
      console.log('not good enough yet text: ' + text);
      return;
    }

  
 	  var resolve = this._speechList[0];
    if (this.match_result) {
      this.current_utterance = this.match_result;
    } else {
      this.current_utterance = text;      
    }
    // reset match_result.
    this.match_result = null;

    this.temp_speech = text;
 	  console.log('current utterance set to: ' + this.current_utterance);
 	  for (var i = 0; i < this._speechList.length; i++) {
  	  var resFn = this._speechList[i];
  	  resFn();
 	  }
    this._playSound(this._endSoundBuffer);
 	  // Pause the mic and close the web socket.
 	  this._context.suspend.bind(this._context);
 	  this._closeWebsocket();

 	  this._speechList = [];
 	  this._alreadyListening = false;
    if (this._speechTimeout) {
      clearTimeout(this._speechTimeout);
      this._speechTimeout = null;
    }
    // timeout for waiting for last result.
    if (this._speechTimeoutResponseTimeout) {
      clearTimeout(this._speechTimeoutResponseTimeout);
      this._speechTimeoutResponseTimeout = null;
    }
 }

 _computeMatch(needle, haystack) {
   //var text = this._phrase_list.join(' ');

   // Don't bother matching if any are null.
   if (!needle || !haystack) {
     return -1;
   }

   var loc = 0;

   var match = this.match_main(haystack, needle, loc);
   if (match == -1) {
    //console.log('no match');
   } else {
     var quote = haystack.substring(match, match + haystack.length);
//     console.log(' match found at character  ' + match + ' ' + quote);
   }
   return match;
 }

 // Disconnect all the audio stuff on the client.
 _suspendListening() {
 	  console.log('suspending listenting');
 	  // this gets called on green flag when context may not exist yet.
 	  if (this._context) {
 	  	  console.log('suspending audio context.')
   	  this._context.suspend.bind(this._context);
 		  this._scriptNode.disconnect();
 	  }
    if (this._sourceNode) this._sourceNode.disconnect();
 }

 // This needs a new name - currently handles all messages fromt the socket 
 // server. Even the init message and the "end of utterance message";
  _onTranscriptionFromServer(e) {
    console.log('transcription ' + e.data);
    if (e.data == 'got the configuration message') {
    	console.log('received initial response from socket server.');
    	return;
    } else if (e.data == 'end of utterance') {
    	// End of utterance is a message we get, but it doesn't mean we've got
    	// the final results yet.  So for now, ignore?
    	console.log('Got an end of utterance message. Ignoring it though.');
    	return;
    }

    // This is an actual transcription result.
    try {
    	var result = JSON.parse(e.data);
    } catch (ex) {
    	console.log ('problem parsing json. continuing: ' + ex);
    	// TODO: stop stuff?
    	return;
    }
    // Throw a transcription event that we'll catch later and decice whether to
    // resolve the promise.  
    //      this.runtime.emit('TRANSCRIPTION', result);
    this._onTranscription(result);
  }

  // Called when we have data from the Microphone. Takes that data and ships
  // it off to the speech server for transcription.
  _processAudioCallback(e) {
    if (this._socket.readyState == WebSocket.CLOSED ||
        this._socket.readyState == WebSocket.CLOSING) {
      console.log('Not sending data because not in ready state. State: ' + this._socket.readyState);
      return;
    }
    const MAX_INT = Math.pow(2, 16 - 1) - 1;
    var floatSamples = e.inputBuffer.getChannelData(0);
   	// The samples are floats in range [-1, 1]. Convert to 16-bit signed
    // integer.
    this._socket.send(Int16Array.from(floatSamples.map(function(n) {
      return n * MAX_INT;
     })));
  }

  // Called to setup the AudioContext and its callbacks.
  initWebSocket() {
    // Create a node that sends raw bytes across the websocket
    this._scriptNode = this._context.createScriptProcessor(4096, 1, 1);
    // Need the maximum value for 16-bit signed samples, to convert from float.
    this._scriptNode.addEventListener('audioprocess', this._processAudioCallback);
    this._newWebsocket();
  }

  // Called when we're ready to start recording from the microphone and sending
  // that data to the speech server.
  startRecording() {
    if (this._context) {
  	  console.log('Already did the setup. Trying to resume.');
  	  this._context.resume.bind(this._context);
  	  this._newWebsocket();
  	  return;
  	}
	 	console.log('starting recording');
  	// All the setup for reading from microphone.
	 	this._context = new AudioContext();
	 	// TODO: put these constants elsewhere
	 	const SAMPLE_RATE = 16000;
		const SAMPLE_SIZE = 16;
		this._audioPromise = navigator.mediaDevices.getUserMedia({
	      audio: {
	        echoCancellation: true,
	        channelCount: 1,
	        sampleRate: {
	          ideal: SAMPLE_RATE
	        },
	        sampleSize: SAMPLE_SIZE
	      }
	    });
		var tempContext = this._context;
		var analyser;
	    this._audioPromise.then(function(micStream) {
	      var microphone = tempContext.createMediaStreamSource(micStream);
	      analyser = tempContext.createAnalyser();
	      microphone.connect(analyser);
	    }).catch(console.log.bind(console));
	   
	  this.initWebSocket();
	}

  

  /**
   * @returns {object} metadata for this extension and its blocks.
   */
  getInfo () {
    return {
            id: 'speech',
            name: 'Google Speech',
            menuIconURI: menuIconURI,
            blockIconURI: iconURI,
            blocks: [
                {
                    opcode: 'listenAndWait',
                    text: 'Listen and Wait',
                    blockType: BlockType.COMMAND,
                },
                {
                    opcode: 'whenIHearHat',
                    text: 'When I hear [PHRASE]',
                    blockType: BlockType.HAT,
                    arguments : {
                      PHRASE: {
                        type: ArgumentType.STRING,
                        defaultValue: 'cat'
                      }
                    }
                },
                {
                    opcode: 'getSpeech',
                    text: 'speech',
                    blockType: BlockType.REPORTER
                },
            ],
        };
  }
	
  _speechMatches(needle, haystack) {
    let input = Cast.toString(needle).toLowerCase();
    // facilitate matches by removing some punctuation: . ? !
    input = input.replace(/[.?!]/g, '');
    // trim off any white space
    input = input.trim();

    var match = this._computeMatch(needle, haystack);
    return match != -1;
    // if (haystack && haystack.indexOf(input) != -1) {
    //   return true;
    // }
    // return false;
  }

	
  whenIHearHat(args) {
    return this._speechMatches(args.PHRASE, this.temp_speech);
  }

  _showIndicator() {
    console.log('i should show a thing');
  }

	listenAndWait(args) {
     this._playSound(this._startSoundBuffer);
     this._showIndicator();
      this._scanBlocksForPhraseList();
      this.temp_speech = '';
      var speechPromise = new Promise(resolve => {
         var listeningInProgress = this._speechList.length > 0;
         this._speechList.push(resolve);
         if (!listeningInProgress) {
         	this._startNextListening();
         }
      });
      return speechPromise;
    }


  _playSound(buffer) {
    if (this.runtime.audioEngine === null) return;
        const context = this.runtime.audioEngine.audioContext;
        const bufferSource = context.createBufferSource();
        bufferSource.buffer = buffer;
        bufferSource.connect(this.runtime.audioEngine.input);
        bufferSource.start();
    }

  // Kick off listening for the next block waiting in the queue.
  _startNextListening() {
    console.log('start listening');
    if (this._speechList.length > 0) {
        this.startRecording();
        this._alreadyListening = true;
      // 10 second timeout for listening.
      this._speechTimeout = setTimeout(this._timeOutListening, 10000);
    } else {
      	console.log('trying to start listening but for no reason?');
    }
  }

  // Reporter for the last heard phrase/utterance.
  getSpeech(args) {
    return this.current_utterance;
  }

  

};
module.exports = Scratch3SpeechBlocks;
